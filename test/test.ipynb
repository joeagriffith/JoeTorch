{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "from joetorch.datasets import MNIST\n",
    "from joetorch.nn import *\n",
    "from joetorch.optim import *\n",
    "from joetorch.logging import get_writer\n",
    "\n",
    "# Training Hyperparameters\n",
    "experiment_name = 'mnist'\n",
    "out_dir = 'test/out/'\n",
    "num_epochs = 50\n",
    "batch_size = 256\n",
    "start_lr, end_lr = 1e-3, 1e-4\n",
    "lr_warmup_epochs = 10\n",
    "start_wd, end_wd = 4e-3, 4e-2\n",
    "epoch_hyperparams = {\n",
    "    'lr': cosine_schedule(base=start_lr, end=end_lr, T=num_epochs, warmup=lr_warmup_epochs),\n",
    "    'wd': cosine_schedule(base=start_wd, end=end_wd, T=num_epochs),\n",
    "}\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "root = '../../datasets/'\n",
    "val_ratio = 0.1\n",
    "train_dataset = MNIST(root=root, split='train', val_ratio=val_ratio, augment=True, dtype=dtype, device=device, download=False)\n",
    "val_dataset = MNIST(root=root, split='val', val_ratio=val_ratio, dtype=dtype, device=device, download=False)\n",
    "test_dataset = MNIST(root=root, split='test', dtype=dtype, device=device, download=False)\n",
    "\n",
    "# MLP trial\n",
    "trial_name = 'mlp_ae_mseloss'\n",
    "model = MNIST_AE(out_dim=20, mode='mlp').to(device)\n",
    "model = MNIST_AE(out_dim=20, mode='cnn').to(device)\n",
    "optimiser = get_optimiser(model, optim='AdamW')\n",
    "writer = get_writer(out_dir, experiment_name, trial_name)\n",
    "save_dir = out_dir + f'{experiment_name}/models/{trial_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from joetorch.datasets.dataset import PreloadedDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Optional, Union, List, Dict, Tuple\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(\n",
    "        model: torch.nn.Module,\n",
    "        train_dataset: PreloadedDataset,\n",
    "        optimiser: torch.optim.Optimizer,\n",
    "        num_epochs: int,\n",
    "        batch_size: int,\n",
    "        val_dataset: Optional[PreloadedDataset] = None,\n",
    "        writer: Optional[SummaryWriter] = None,\n",
    "        compute_dtype: str = 'float32',\n",
    "        compile: bool = False,\n",
    "        epoch_hyperparams: Dict[str, any] = {},\n",
    "        loss_args: Dict[str, any] = {},\n",
    "        save_dir: Optional[str] = None,\n",
    "        save_metric: Tuple[str, str, str] = ('minimise', 'val', 'loss'),\n",
    "        save_every: Optional[int] = None,\n",
    "        save_last: bool = False,\n",
    "        target_networks: Optional[List[Tuple[torch.nn.Module, torch.nn.Module, torch.Tensor]]] = None,\n",
    "        max_grad_norm: Optional[float] = None,\n",
    "        grad_norm_depth: int = 0,\n",
    "        **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a model for a given number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model: The model to train.\n",
    "        train_dataset: The training dataset.\n",
    "        val_dataset: The validation dataset.\n",
    "        optimiser: The optimiser to use.\n",
    "        num_epochs: The number of epochs to train for.\n",
    "        batch_size: The batch size.\n",
    "        writer: The writer to use for logging.\n",
    "        mixed_precision: Whether to use mixed precision.\n",
    "        epoch_hyperparams: The hyperparameters to use for each epoch.\n",
    "        loss_args: Extra arguments to pass to the loss function.\n",
    "        save_dir: The directory to save the model.\n",
    "        save_metric: Tuple of ('minimise'/'maximise', 'train'/'val', {metric}).\n",
    "    \"\"\"\n",
    "    assert save_metric[0] in ['minimise', 'maximise']\n",
    "    assert save_metric[1] in ['train', 'val']\n",
    "\n",
    "    if compile:\n",
    "        loss_fn = torch.compile(model.loss)\n",
    "    else:\n",
    "        loss_fn = model.loss\n",
    "\n",
    "    compute_dtype = getattr(torch, str(compute_dtype).split('.')[-1])\n",
    "    compute_device = next(model.parameters()).device\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = None if val_dataset is None else DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    loop = tqdm(range(num_epochs), desc='Epochs', leave=False)\n",
    "    best_metric = torch.inf if save_metric[0] == 'minimise' else -torch.inf\n",
    "    for epoch in loop:\n",
    "\n",
    "        # ========================== HYPERPARAMETER UPDATE ==========================\n",
    "\n",
    "        for key, value in epoch_hyperparams.items():\n",
    "            if key == 'lr':\n",
    "                for param_group in optimiser.param_groups:\n",
    "                    param_group['lr'] = value[epoch]\n",
    "            elif key == 'wd':\n",
    "                for param_group in optimiser.param_groups:\n",
    "                    if param_group['weight_decay'] != 0.0:\n",
    "                        param_group['weight_decay'] = value[epoch]\n",
    "            \n",
    "        epoch_loss_args = {}\n",
    "        for key, value in loss_args.items():\n",
    "            if (isinstance(value, torch.Tensor) or isinstance(value, np.ndarray) or isinstance(value, list)) and len(value) == num_epochs:\n",
    "                epoch_loss_args[key] = value[epoch]\n",
    "            else:\n",
    "                epoch_loss_args[key] = value\n",
    "                    \n",
    "        # ============================ TRAINING ============================\n",
    "        if train_dataset.transform is not None:\n",
    "            train_dataset.update_transformed_images()\n",
    "\n",
    "        model.train()\n",
    "        epoch_train_metrics = {}\n",
    "        for batch in train_loader:\n",
    "            optimiser.zero_grad()\n",
    "            with torch.autocast(device_type=compute_device.type, dtype=compute_dtype, enabled=compute_dtype is not None):\n",
    "                if 'train_metrics' in locals():\n",
    "                    del train_metrics\n",
    "                    torch.cuda.empty_cache()\n",
    "                train_metrics = loss_fn(batch, **epoch_loss_args)\n",
    "                train_metrics['loss'].backward()\n",
    "\n",
    "                if grad_norm_depth == 0:\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                    train_metrics[f'{model.__class__.__name__}_grad_norm'] = grad_norm\n",
    "                else:\n",
    "                    def get_submodules(module, depth, prefix=''):\n",
    "                        if depth == 0:\n",
    "                            return [(prefix.rstrip('.'), module)]\n",
    "                        \n",
    "                        submods = []\n",
    "                        for name, submod in module.named_children():\n",
    "                            submod_name = f\"{prefix}{name}.\"\n",
    "                            submods.extend(get_submodules(submod, depth-1, submod_name))\n",
    "                        return submods if submods else [(prefix.rstrip('.'), module)]\n",
    "\n",
    "                    for name, submod in get_submodules(model, grad_norm_depth):\n",
    "                        if any(p.grad is not None for p in submod.parameters()):\n",
    "                            grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                                submod.parameters(), \n",
    "                                max_grad_norm\n",
    "                            )\n",
    "                            train_metrics[f'{name}_grad_norm'] = grad_norm\n",
    "\n",
    "\n",
    "                if max_grad_norm is not None:\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "\n",
    "\n",
    "                optimiser.step()\n",
    "            \n",
    "            for key, value in train_metrics.items():\n",
    "                if key not in epoch_train_metrics:\n",
    "                    epoch_train_metrics[key] = []\n",
    "                epoch_train_metrics[key].append(value.detach().cpu().item())\n",
    "\n",
    "        for key, value in epoch_train_metrics.items():\n",
    "            epoch_train_metrics[key] = np.mean(value)\n",
    "\n",
    "        if writer is not None:\n",
    "            for key, value in epoch_train_metrics.items():\n",
    "                writer.add_scalar(f'train/{key}', value, epoch)\n",
    "            \n",
    "        postfix = {key: round(value.item(), 3) for key, value in epoch_train_metrics.items()}\n",
    "        # ============================ VALIDATION ============================\n",
    "\n",
    "        if val_loader is not None:\n",
    "            if val_dataset.transform is not None:\n",
    "                val_dataset.update_transformed_images()\n",
    "\n",
    "            model.eval()\n",
    "            epoch_val_metrics = {}\n",
    "            for batch in val_loader:\n",
    "                with torch.no_grad():\n",
    "                    with torch.autocast(device_type=compute_device.type, dtype=compute_dtype, enabled=compute_dtype is not None):\n",
    "                        val_metrics = loss_fn(batch, **epoch_loss_args)\n",
    "\n",
    "                for key, value in val_metrics.items():\n",
    "                    if key not in epoch_val_metrics:\n",
    "                        epoch_val_metrics[key] = []\n",
    "                    epoch_val_metrics[key].append(value.detach().cpu().item())\n",
    "            \n",
    "            # ============================ LOGGING ============================\n",
    "\n",
    "            # average metrics over the epoch\n",
    "            for key, value in epoch_val_metrics.items():\n",
    "                epoch_val_metrics[key] = np.mean(value)\n",
    "\n",
    "            if writer is not None:\n",
    "                for key, value in epoch_val_metrics.items():\n",
    "                    writer.add_scalar(f'val/{key}', value, epoch)\n",
    "            \n",
    "            for val_key, val_value in epoch_val_metrics.items():\n",
    "                val_value = round(val_value.item(), 3)\n",
    "                if val_key in postfix:\n",
    "                    train_value = postfix[val_key]\n",
    "                    postfix[val_key] = (train_value, val_value)\n",
    "                else:\n",
    "                    postfix[val_key] = val_value\n",
    "\n",
    "        loop.set_postfix(postfix)\n",
    "\n",
    "        # ============================ TARGET NETWORK UPDATE ============================\n",
    "\n",
    "        if target_networks is not None:\n",
    "            for target_network, network, taus in target_networks:\n",
    "                for param, target_param in zip(network.parameters(), target_network.parameters()):\n",
    "                    target_param.data.copy_(taus[epoch] * param.data + (1 - taus[epoch]) * target_param.data)\n",
    "\n",
    "        # ============================ SAVING ============================\n",
    "\n",
    "        if save_dir is not None:\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            metrics = epoch_val_metrics if save_metric[1] == 'val' else epoch_train_metrics\n",
    "            if (save_metric[0] == 'minimise' and metrics[save_metric[2]] < best_metric) or (save_metric[0] == 'maximise' and metrics[save_metric[2]] > best_metric):\n",
    "                best_metric = metrics[save_metric[2]]\n",
    "                for f in os.listdir(save_dir):\n",
    "                    if 'best' in f:\n",
    "                        os.remove(os.path.join(save_dir, f))\n",
    "                torch.save(model.state_dict(), os.path.join(save_dir, f'best_{save_metric[1]}_{save_metric[2]}_{epoch}.pth'))\n",
    "            if save_every is not None and epoch % save_every == 0:\n",
    "                torch.save(model.state_dict(), os.path.join(save_dir, f'checkpoint_{epoch}.pth'))\n",
    "            elif save_last:\n",
    "                for f in os.listdir(save_dir):\n",
    "                    if 'recent' in f:\n",
    "                        os.remove(os.path.join(save_dir, f))\n",
    "                torch.save(model.state_dict(), os.path.join(save_dir, f'recent_{epoch}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test/out/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_hyperparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch_hyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 100\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataset, optimiser, num_epochs, batch_size, val_dataset, writer, compute_dtype, compile, epoch_hyperparams, loss_args, save_dir, save_metric, save_every, save_last, target_networks, max_grad_norm, grad_norm_depth, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m train_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grad_norm_depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 100\u001b[0m     grad_norm \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     train_metrics[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_grad_norm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m grad_norm\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/ml-env/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py:30\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ml-env/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py:70\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     68\u001b[0m     parameters \u001b[38;5;241m=\u001b[39m [parameters]\n\u001b[1;32m     69\u001b[0m grads \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m---> 70\u001b[0m max_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m norm_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(norm_type)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(grads) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "train(model, train_dataset, optimiser, num_epochs, batch_size, val_dataset, writer, compute_dtype=dtype, epoch_hyperparams=epoch_hyperparams, save_dir=save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
